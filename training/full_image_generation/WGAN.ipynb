{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgvPdZGQRHby"
   },
   "source": [
    "## Notebook that trains a Wasserstein-GAN model for galaxy generation\n",
    "\n",
    "Adapted from: https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/wgan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCuk8b4VX9tt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# set up accordingly\n",
    "data_dir = \"../../data\" # directory with data files\n",
    "labeled_image_dir = \"labeled\"       # folder within data directory with labeled images (0, 1)\n",
    "scored_image_dir = \"scored_128\"      # folder within data directory with scored images\n",
    "device = torch.device(\"cuda\")        # cuda or cpu\n",
    "collab = False                        # google collab flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "N4LUY-KdYY3q",
    "outputId": "3eca31bc-8baa-4cff-9f21-1a61ccc9ec98"
   },
   "outputs": [],
   "source": [
    "# google collab\n",
    "# mount drive, copy over data as zip and unzip it\n",
    "if collab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  collab_dir = \"content\"\n",
    "\n",
    "  zip_path = os.path.join(data_dir, 'labeled.zip')\n",
    "  !cp '{zip_path}' .\n",
    "  !unzip -q labeled.zip\n",
    "  !rm labeled.zip\n",
    "\n",
    "  zip_path = os.path.join(data_dir, 'scored_128.zip')\n",
    "  !cp '{zip_path}' .\n",
    "  !unzip -q scored_128.zip\n",
    "  !rm scored_128.zip\n",
    "else:\n",
    "    labeled_image_dir = os.path.join(data_dir, labeled_image_dir)\n",
    "    scored_image_dir = os.path.join(data_dir, scored_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8HEJnMGYizQ"
   },
   "outputs": [],
   "source": [
    "class GalaxyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Galaxy dataset class\n",
    "    Builds a dataset from the labeled and scored images. \n",
    "    Requires a threshold score for scored images. \n",
    "    Images with a score below the threshold are not used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, image_dir, scored_dir=None, scores_file=None, transform=None, train=True, size=(128, 128), train_split=0.8, scored_threshold=0):\n",
    "        self.labels = pd.read_csv(csv_file, index_col=\"Id\")\n",
    "        self.labels = self.labels[self.labels['Actual'] == 1.0]\n",
    "        self.size = size\n",
    "        self.scores = None\n",
    "        if scores_file is not None:\n",
    "          self.scores = pd.read_csv(scores_file, index_col=\"Id\")\n",
    "        self.samples = []\n",
    "        if train == True:\n",
    "          self.labels = self.labels[:int(self.labels.shape[0]*train_split)]\n",
    "        else:\n",
    "          self.labels = self.labels[int(self.labels.shape[0]*train_split):]\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.scored_dir = scored_dir\n",
    "        self.scored_threshold = scored_threshold\n",
    "        self.load_dataset()\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_dataset(self):\n",
    "      print(\"Loading Dataset...\")\n",
    "      for id, _ in self.labels.iterrows():\n",
    "        img_name = os.path.join(self.image_dir,\n",
    "                                  str(id)+'.png')\n",
    "        self.samples.append(Image.open(img_name).resize(self.size))\n",
    "      \n",
    "      if self.scores is not None:\n",
    "        for id, score in self.scores.iterrows():\n",
    "          if score.item() > self.scored_threshold:\n",
    "\n",
    "            img_name = os.path.join(self.scored_dir,\n",
    "                                      str(id)+'.png')\n",
    "            self.samples.append(Image.open(img_name).resize(self.size))\n",
    "        \n",
    "      print(\"Dataset Loaded\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.samples[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFy8wiEYMXsf"
   },
   "source": [
    "Load Dataset with appropriate transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "qKjOe4WWZlFO",
    "outputId": "18d894d8-1932-499d-ddad-7bb939639e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Dataset Loaded\n",
      "2754 images loaded\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "size = (64, 64)\n",
    "latent_dim = 64\n",
    "train_transformation = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.RandomAffine(30, translate=(0.2, 0.2)),\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(0, 255.0),\n",
    "                            torchvision.transforms.Normalize(0.5, 0.5) ## second normalization for tanh\n",
    "])\n",
    "val_transformation = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.RandomAffine(30, translate=(0.2, 0.2)),\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(0, 255.0),\n",
    "                            torchvision.transforms.Normalize(0.5, 0.5) ## second normalization for tanh\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = GalaxyDataset(os.path.join(data_dir, \"labeled.csv\"), labeled_image_dir, scored_dir=scored_image_dir, scores_file=os.path.join(data_dir, \"scored.csv\"), transform=train_transformation, train=True, size=size, train_split=1.0, scored_threshold=2.60)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=1, pin_memory=True)\n",
    "print(\"{} images loaded\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bbb7MWg0Meu9"
   },
   "source": [
    "Set up initialization of weights, filters of generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUjdbEcrbfeq"
   },
   "outputs": [],
   "source": [
    "gen_base_filters = 64\n",
    "disc_base_filters = 64\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci_ZibcbZsKF"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( latent_dim, gen_base_filters * 8, 4, 1, 0, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(gen_base_filters * 8),\n",
    "            # state size. (gen_base_filters*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(gen_base_filters * 8, gen_base_filters * 4, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(gen_base_filters * 4),\n",
    "            # state size. (gen_base_filters*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( gen_base_filters * 4, gen_base_filters * 2, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(gen_base_filters * 2),\n",
    "            # state size. (gen_base_filters*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( gen_base_filters * 2, gen_base_filters, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(gen_base_filters),\n",
    "            # state size. (gen_base_filters) x 32 x 32\n",
    "            nn.ConvTranspose2d( gen_base_filters, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "c-jgXpRfd-8T",
    "outputId": "852aa4d9-9d6f-432a-f589-676f6276243b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gen = Generator().to(device)\n",
    "gen.apply(weights_init)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmyTA_b9edTY"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(1, disc_base_filters, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (disc_base_filters) x 32 x 32\n",
    "            nn.Conv2d(disc_base_filters, disc_base_filters * 2, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(disc_base_filters * 2),\n",
    "            # state size. (disc_base_filters*2) x 16 x 16\n",
    "            nn.Conv2d(disc_base_filters * 2, disc_base_filters * 4, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(disc_base_filters * 4),\n",
    "            # state size. (disc_base_filters*4) x 8 x 8\n",
    "            nn.Conv2d(disc_base_filters * 4, disc_base_filters * 8, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(disc_base_filters * 8),\n",
    "            # state size. (disc_base_filters*8) x 4 x 4\n",
    "            nn.Conv2d(disc_base_filters * 8, 1, 4, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "zKk2jgq8faJN",
    "outputId": "262ca704-f582-4053-9dfc-a521a17d9ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator().to(device)\n",
    "disc.apply(weights_init)\n",
    "print(disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP7JiGaCMlm-"
   },
   "source": [
    "Set up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9w63CPzWfpDk"
   },
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n",
    "\n",
    "gen_lr = 0.00005\n",
    "disc_lr = 0.00005\n",
    "epochs = 2000\n",
    "clip_value = 0.01\n",
    "n_critic = 5\n",
    "# Setup RMSprop optimizers for both G and D\n",
    "optimizerD = optim.RMSprop(disc.parameters(), lr=disc_lr)\n",
    "optimizerG = optim.RMSprop(gen.parameters(), lr=gen_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xvgTs-LMnmQ"
   },
   "source": [
    "Function that takes one WGAN train step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2PPqxMiSdEq"
   },
   "outputs": [],
   "source": [
    "def wgan_step(batch_num, batch, disc, gen):\n",
    "  ############################\n",
    "  # (1) Update D network\n",
    "  ###########################\n",
    "  disc.zero_grad()\n",
    "  # Format batch\n",
    "  real = data.to(device)\n",
    "  b_size = real.size(0)\n",
    "  # Generate batch of latent vectors\n",
    "  noise = torch.randn(b_size, latent_dim, 1, 1, device=device)\n",
    "  # Generate fake image batch with G\n",
    "  fake = gen(noise)\n",
    "  loss_D = -torch.mean(disc(real)) + torch.mean(disc(fake))\n",
    "  loss_D.backward()\n",
    " \n",
    "\n",
    "  ############################\n",
    "  # (2) Update G network\n",
    "  ###########################\n",
    "  # Train the generator every n_critic iterations\n",
    "  if batch_num % n_critic == 0:\n",
    "\n",
    "    gen.zero_grad()\n",
    "\n",
    "    # Generate a batch of images\n",
    "    gen_imgs = gen(noise)\n",
    "    # Adversarial loss\n",
    "    loss_G = -torch.mean(disc(gen_imgs))\n",
    "\n",
    "    loss_G.backward()\n",
    "\n",
    "    # Output training stats\n",
    "    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "          % (epoch, epochs, i, len(train_loader),\n",
    "              loss_D.item(), loss_G.item()))\n",
    "    return loss_D, loss_G\n",
    "  return loss_D, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVwS_Z3xMsKF"
   },
   "source": [
    "Train and periodically save outputs from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbJx9U4_f6Fa"
   },
   "outputs": [],
   "source": [
    "# save a grid of images generated from the same initial noise\n",
    "# also save those same images but changed so that the smallest value\n",
    "# corresponds to 0, and the largest to 1 (normalized)\n",
    "# this allows us to see some patterns that would otherwise\n",
    "# just appear as black\n",
    "results_dir = \"results_wgan\"\n",
    "if not os.path.exists(os.path.join(outf, results_dir)):\n",
    "  os.mkdir(os.path.join(outf, results_dir))\n",
    "  os.mkdir(os.path.join(outf, results_dir, \"normalized\"))\n",
    "  os.mkdir(os.path.join(outf, results_dir, \"unnormalized\"))\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        errD, errG = wgan_step(i, data, disc, gen)\n",
    "        \n",
    "        if errG is not None:\n",
    "          # Update G\n",
    "          optimizerG.step()\n",
    "          # Save Losses for plotting later\n",
    "          G_losses.append(errG.item())\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        # clip weights\n",
    "        for p in disc.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "        # Save Losses for plotting later\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (epoch % 5 == 0 and i == len(train_loader)-1):\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, pad_value=255, normalize=True, range=(-1, 1)))\n",
    "            # also save normalized so we can see patterns / mode collapse\n",
    "            vutils.save_image(fake,  os.path.join(outf, results_dir, \"normalized\") + \"/epoch\" + str(epoch) + \".png\", padding=2, pad_value=1, normalize=True)\n",
    "            vutils.save_image(fake, os.path.join(outf, results_dir, \"unnormalized\") + \"/epoch\" + str(epoch) + \".png\", padding=2, pad_value=1, normalize=True, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "149wKWUwlDAG"
   },
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), os.path.join(outf, results_dir, \"wgen.model\"))\n",
    "torch.save(disc.state_dict(), os.path.join(outf, results_dir, \"wdisc.model\"))\n",
    "if collab:\n",
    "  !cp '{outf}'/'{results_dir}'wgen.model '{data_dir}'/'{results_dir}'\n",
    "  !cp '{outf}'/'{results_dir}' wdisc.model '{data_dir}'/'{results_dir}'\n",
    "  !cp -rf /'{outf}'/'{results_dir}' '{data_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JULKASsOMxZn"
   },
   "source": [
    "Plot train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz_wo8TupqlP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehbLdRqyMzOl"
   },
   "source": [
    "Check how images progressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIrEALGIqFED"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
